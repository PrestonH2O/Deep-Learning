{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning\n",
    "\n",
    "#### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, input_size, hidden_size, output_size):        \n",
    "        #Define Hyperparameters\n",
    "        self.input_layer_size = input_size\n",
    "        self.hidden_layer_size = hidden_size\n",
    "        self.output_layer_size = output_size\n",
    "        \n",
    "        # Weights\n",
    "        self.weight_input_hidden = np.random.randn(self.input_layer_size,self.hidden_layer_size)\n",
    "        self.weight_hidden_output = np.random.randn(self.hidden_layer_size,self.output_layer_size)\n",
    "\n",
    "        # Biases\n",
    "        self.biases_input_hidden = np.zeros((1, self.hidden_layer_size))\n",
    "        self.biases_hidden_output = np.zeros((1, self.output_layer_size))\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Sigmoid activation function\n",
    "        return 1/(1+np.exp(-z))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        # Derivative of the sigmoid activation function\n",
    "        # Assumes x is the output of the sigmoid function\n",
    "        return x * (1 - x)\n",
    "        \n",
    "    def forward_propagation(self, inputs):\n",
    "        # Propagate inputs through network\n",
    "        # Takes inputs which should be [ n x input_layer_size]\n",
    "        self.hidden_input = np.dot(inputs, self.weight_input_hidden) + self.biases_input_hidden\n",
    "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
    "        self.output_input = np.dot(self.hidden_output, self.weight_hidden_output) + self.biases_hidden_output\n",
    "        self.output_output = self.sigmoid(self.output_input) \n",
    "\n",
    "        \n",
    "    def backward_propagation(self, inputs, output, learn_rate):\n",
    "        #Compute derivative with respect to each weight matrix for a given input output pair\n",
    "        self.forward_propagation(inputs)\n",
    "        \n",
    "        delta2 = np.multiply(-(output-self.output_output), self.sigmoid_derivative(self.output_output))\n",
    "        d_W2 = np.dot(self.hidden_output.T, delta2)\n",
    "        d_bias2 = np.sum(delta2, axis=0)\n",
    "        \n",
    "        delta1 = np.dot(delta2, self.weight_hidden_output.T)*self.sigmoid_derivative(self.hidden_output)\n",
    "        d_W1 = np.dot(inputs.T, delta1)\n",
    "        d_bias1 = np.sum(delta1, axis=0)\n",
    "            \n",
    "        self.weight_input_hidden += (learn_rate * d_W1)\n",
    "        self.biases_input_hidden += (learn_rate * d_bias1)\n",
    "        self.weight_hidden_output += (learn_rate * d_W2)\n",
    "        self.biases_hidden_output += (learn_rate * d_bias2)\n",
    "\n",
    "    def train(self, inputs, outputs, epochs, learn_rate):\n",
    "        # Train the neural network using gradient descent \n",
    "        for epoch in range(epochs):\n",
    "            self.backward_propagation(inputs, outputs, learn_rate)\n",
    "\n",
    "\n",
    "    def train_miniBatch(self, inputs, outputs, epochs, learn_rate, batch_size):\n",
    "        # Determine the number of batches\n",
    "        num_batches = len(inputs) // batch_size\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle the data at the beginning of each epoch\n",
    "            indices = np.random.permutation(len(inputs))\n",
    "            shuffled_inputs = inputs[indices]\n",
    "            shuffled_outputs = outputs[indices]\n",
    "\n",
    "            # Loop through each mini-batch\n",
    "            for batch_id in range(num_batches):\n",
    "                # Extract the current mini-batch\n",
    "                start_id = batch_id * batch_size\n",
    "                end_id = (batch_id + 1) * batch_size\n",
    "                mini_batch_inputs = shuffled_inputs[start_id:end_id]\n",
    "                mini_batch_outputs = shuffled_outputs[start_id:end_id]\n",
    "\n",
    "                # Perform backward propagation on the mini-batch\n",
    "                self.backward_propagation(mini_batch_inputs, mini_batch_outputs, learn_rate)\n",
    "\n",
    "    \n",
    "    def cost(self, inputs, outputs):\n",
    "        # Calculate the loss function\n",
    "        self.forward_propagation(inputs)\n",
    "        return 0.5 * np.mean(np.square(outputs - self.output_output))\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        # Make predictions for the test set\n",
    "        self.forward_propagation(inputs)\n",
    "        return self.output_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and process data\n",
    "\n",
    "Data source: https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset\n",
    "- instant: record index\n",
    "- dteday : date\n",
    "- season : season (1:springer, 2:summer, 3:fall, 4:winter)\n",
    "- yr : year (0: 2011, 1:2012)\n",
    "- mnth : month ( 1 to 12)\n",
    "- hr : hour (0 to 23)\n",
    "- holiday : weather day is holiday(1) or not(0) (extracted from http://dchr.dc.gov/page/holiday-schedule)\n",
    "- weekday : day of the week (0-6) (sunday-saturday)\n",
    "- workingday : if day is neither weekend nor holiday is 1, otherwise is 0.\n",
    "+ weathersit : \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n",
    "- temp : Normalized temperature in Celsius. The values are divided to 41 (max)\n",
    "- atemp: Normalized feeling temperature in Celsius. The values are divided to 50 (max)\n",
    "- hum: Normalized humidity. The values are divided to 100 (max)\n",
    "- windspeed: Normalized wind speed. The values are divided to 67 (max)\n",
    "- casual: count of casual users\n",
    "- registered: count of registered users\n",
    "- cnt: count of total rental bikes including both casual and registered\n",
    "\n",
    "This is a regression problem, we will try to use a neural network to establish a relationship between cnt (the number of bikes rented) and the other variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of bikes rented: \n",
      "max:  977\n",
      "min 1\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('hour.csv')\n",
    "#df = df.head(10)\n",
    "\n",
    "# input_data is the input data in the shape (m x n) aka (examples x features) \n",
    "# Dropping some irrelevant / ill formatted columns \n",
    "input_data = df.drop(columns=['cnt']).drop(columns=['instant']).drop(columns=['dteday']).values\n",
    "\n",
    "# output_data is true/target values (number of bikes rented)\n",
    "output_data = df['cnt'].values\n",
    "\n",
    "# Normalize data\n",
    "transformer = Normalizer(norm='max').fit(input_data)\n",
    "input_data = transformer.transform(input_data)\n",
    "\n",
    "# Normalizing 1D output data\n",
    "highest_val = output_data.max()\n",
    "lowest_val = output_data.min()\n",
    "print(\"Range of bikes rented: \")\n",
    "print(\"max: \", highest_val)\n",
    "print(\"min\", lowest_val)\n",
    "output_data = (output_data - lowest_val) / (highest_val - lowest_val)\n",
    "output_data = output_data.reshape(-1, 1)\n",
    "# Scaling output so the sigmoid used will actually get the right answer.  \n",
    "# When we care what the real answer is, just undo the normalization.\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "train_input, test_input, train_output, test_output = train_test_split(input_data, output_data, test_size=0.3) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost: 0.006253569325818177\n",
      "Range of bikes rented predictions: \n",
      "predicted max:  416.0\n",
      "predicted min:  1.0\n",
      "MAE:  67.00805523590334\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize neural network\n",
    "input_size = np.shape(input_data)[1] # Each input node is one feature of the data\n",
    "hidden_size = 8 # Arbitrary number of hidden nodes\n",
    "output_size = 1 # Looking for an output that a single value\n",
    "learning_rate = -0.001\n",
    "epochs = 1000\n",
    "\n",
    "# Create an instance of the NeuralNetwork class\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "nn.train(train_input, train_output, epochs, learning_rate)\n",
    "\n",
    "# Calculate the cost after training\n",
    "training_cost = nn.cost(train_input, train_output)\n",
    "print(\"Training cost:\", training_cost)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = nn.predict(test_input)\n",
    "\n",
    "# Returning results to original range\n",
    "predictions_whole = ((predictions) * (highest_val - lowest_val)) + lowest_val\n",
    "# You cannot rent 1/2 a bike.\n",
    "predictions_whole = np.round(predictions_whole)\n",
    "\n",
    "print(\"Range of bikes rented predictions: \")\n",
    "print(\"predicted max: \", predictions_whole.max())\n",
    "print(\"predicted min: \", predictions_whole.min())\n",
    "\n",
    "# Calculate the mean of absolute errors\n",
    "test_output_whole = test_output * (highest_val - lowest_val)\n",
    "mae = np.mean(np.abs(predictions_whole - test_output_whole))\n",
    "print(\"MAE: \", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of results \n",
    "\n",
    "WHOA! That result is very wrong... \n",
    "\n",
    "Why though? Well, I'll be honest, I don't know. I re-coded this twice and made no progress solving the issue. It seems that the delta values added to the weights are *always* positive so the weights and biases just converge to their maximum value of 1 rather than performing gradient descent as expected. The only thing that seems to help is making the learning rate negative. I don't think this indicates I missed a negative sign somewhere, though it is entirely possible. I think this just causes the wrong answer to be found slower so the less wrong answer of essentially random weights is still better. \n",
    "\n",
    "Here's what the issue probably isn't:\n",
    "\n",
    "Data set: I tried an alternate dataset to make sure that wasn't the problem and it did not help.\n",
    "\n",
    "Hyperparamers: Shrinking the learning rate just slows the speed at which our weights approach their maximum. Increasing the epochs just gives more time to approach their maximum. For this reason, setting both values to zero seems to be the best since random guessing on an untrained network is better than definitely wrong.\n",
    "\n",
    "\n",
    "#### Explanation of choices:\n",
    "\n",
    "Fortunately, the way this is coded at least gets the wrong answer pretty darn fast. Thank god for small miracles... Using matices allows efficient computation without any additional efficiency measures. \n",
    "\n",
    "It is more complicated and less efficient to use mini-batch gradient descent. Since linear algebra is really efficent, making the matrixes smaller shouldn't make a marked difference but being forced to train for epochs*batches rather than just epochs causes a non insignifigant slow down. To demonstrait this, batch gradient decent is presented below. Not that with 10 batches, it is about 10 times slower.\n",
    "\n",
    "\n",
    "This series of videos was really fantastic for explaining the ideas and coding pricipals behind making a neural network from scratch.\n",
    "https://www.youtube.com/watch?v=bxe2T-V8XRs&list=PLiaHhY2iBX9hdHaRr6b7XevZtgZRa1PoU&index=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training cost: 0.0034250489506197964\n",
      " predicted max:  549.0\n",
      " predicted min:  1.0\n",
      "MAE:  53.27752205600307\n"
     ]
    }
   ],
   "source": [
    "# Training mini batch \n",
    "\n",
    "# Initialize neural network\n",
    "input_size = np.shape(input_data)[1] # Each input node is one feature of the data\n",
    "hidden_size = 8 # Arbitrary number of hidden nodes\n",
    "output_size = 1 # Looking for an output that a single value\n",
    "learning_rate = -0.001\n",
    "epochs = 1000\n",
    "\n",
    "# Create an instance of the NeuralNetwork class\n",
    "nn = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "\n",
    "# Train the neural network\n",
    "nn.train_miniBatch(train_input, train_output, epochs, learning_rate, 10)\n",
    "\n",
    "# Calculate the cost after training\n",
    "training_cost = nn.cost(train_input, train_output)\n",
    "print(\"Training cost:\", training_cost)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = nn.predict(test_input)\n",
    "\n",
    "# Returning results to original range\n",
    "predictions_whole = ((predictions) * (highest_val - lowest_val)) + lowest_val\n",
    "# You cannot rent 1/2 a bike.\n",
    "predictions_whole = np.round(predictions_whole)\n",
    "\n",
    "print(\"Range of bikes rented predictions: \")\n",
    "print(\" predicted max: \", predictions_whole.max())\n",
    "print(\" predicted min: \", predictions_whole.min())\n",
    "\n",
    "# Calculate the mean of absolute errors\n",
    "test_output_whole = test_output * (highest_val - lowest_val)\n",
    "mae = np.mean(np.abs(predictions_whole - test_output_whole))\n",
    "print(\"MAE: \", mae)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
